
	access_log   /var/catramms/logs/nginx/mms-webapi.access.log main;
	error_log    /var/catramms/logs/nginx/mms-webapi.error.log error;

	#defines the shared memory zone used for storing the cache keys and other metadata
	proxy_cache webapi_cache;
	proxy_cache_key $scheme://$host$uri$is_args$query_string;
	#default: GET HEAD
	#proxy_cache_methods GET HEAD POST;

	#scenario client url: https://mms-webapi.catramms-cloud.com/....
	location / {
		#Uso una coda "minima" per gestire le richieste contemporanee.
		#Non userei il nodelay in modo che le richieste vengono evase secondo il rate indicato in nginx.conf
		#Nota: parliamo dello stesso IP.
		limit_req zone=mmsWEBAPILimit burst=100 nodelay;

		proxy_set_header X-Forwarded-Host $host;
		proxy_set_header X-Forwarded-Server $host;
        proxy_set_header X-Forwarded-Proto $scheme;
		proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
		proxy_pass http://127.0.0.1:8087;

		#If the request is sent with ?should_bypass_cache=true query param, then the cache would be bypassed.
		#You can also use cookies or HTTP headers instead of query params
		proxy_cache_bypass $arg_should_bypass_cache;

		#specifies cache expiry, which can also be configured dynamically by sending cache headers from your backend service
		#valid and inactive
		#proxy_cache_valid set to 5m and inactive set to 10m
		#       If the first request comes at time T0 minutes and next request comes at T6 minutes. The second request would need to fetched
		#               from the backend service even though the data would still be in the disk as the cache has expired.
		#proxy_cache_valid set to 10m and inactive set to 5m
		#       If the first request comes at time T0 minutes and next request comes at T6 minutes. Even though the cache has not expired,
		#               the data is deleted from the disk so it needs to be fetched from backend service again.
		#Quindi inactive determina quando la cache viene eliminata, Cache non acceduta nel periodo specificato da inactive, viene eliminata
		proxy_cache_valid 200 1m;

		#When the backend service is down or takes too long to respond (proxy_connect_timeout default 60s), we can configure Nginx to serve stale responses instead
		#error - use stale if backend can’t be reached
		#timeout - use stale if backend takes too long to respond
		#http_xxx - use stale if backend returns these status codes
		proxy_cache_use_stale error timeout http_500 http_503 http_429;

		#add a “X-Cache” header to the response, indicating if the cache was missed or hit
		add_header X-Cache $upstream_cache_status;

	}

